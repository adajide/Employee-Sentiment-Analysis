# -*- coding: utf-8 -*-
"""NLP Coursework main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dDvqYE_x28kC5GVLtcrybb6Z4Z5ACJJT

#EMPLOYEE SENTIMENT ANALYSIS
"""

#Install the necessary packages
!pip install scikit-learn
!pip install cython
!pip install --force-reinstall gensim[Cython]
!pip install tensorflow
!pip install vaderSentiment

#Import the necessary libraries
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from collections import Counter
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.preprocessing import LabelEncoder
from random import sample
import gensim
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Bidirectional
from keras.utils import to_categorical
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Input
from keras.callbacks import EarlyStopping
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SimpleRNN, LSTM
from tensorflow.keras import mixed_precision
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

"""# DATA PREPROCESSING
The purpose of this analysis is to conduct a sentiment analysis on the employee feedback to have an overview on how the employees feel about the business. The data preprocessing will follow the data preprocesing process such as handling of missing data, cleaning text, data tokeization, punctation removal.
"""

#Import and load the dataset to conduct the sentiment analysis
df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capgemini_Employee_Reviews.csv')
df

# Understanding the clear structure of the dataset by checking the first few rows
df.head()
#Checking for missing values
print(df.isnull().sum())

#fix missing value issue on the Dislike column by dropping them
df.dropna(subset=['Dislikes', 'Likes', 'Department', 'Job_type'], inplace=True)
print(df.isnull().sum())

#shows the number of columns and rows in the dataset after droping missing values
df.shape

"""# Dataset Tokenization"""

#Here, removing the noise and stopwords from the likes and dislikes columns in the dataset by defining a function

#Get the English stopwords
eng_stopwords=stopwords.words('english')

#Defining function for clean text, convert input to strings before tokenization
#then tokenize the words and remove the stopwords

def clean_text(text):
    text = str(text)
    words_tokens=word_tokenize(text)
    words_tokens=[word.lower() for word in  words_tokens]
    words_tokens=[word for word in words_tokens if word.isalpha()]
    words_tokens=[word for word in words_tokens if word not in eng_stopwords]
    return words_tokens

# Apply to 'Likes' and 'Dislikes' columns
df['Likes']=df['Likes'].apply(clean_text)
df['Dislikes']=df['Dislikes'].apply(clean_text)

# Print the cleaned tokens for the first row
print("Cleaned Likes Tokens:", df['Likes'][0])
print("Cleaned Dislikes Tokens:", df['Dislikes'][0])

#visualize the most frequent words in 'Likes' and 'Dislikes' review from the employees
Likes_text=' '.join([' '.join(review) for review in df['Likes']])
Dislikes_text=' '.join([' '.join(review) for review in df['Dislikes']])

#Using the clean_text function to remove commonly used words that does not add value
clean_text(Likes_text)
clean_text(Dislikes_text)

#Creating a worldcloud for the 'Likes' and 'Dislikes' column
Likes_wordcloud=WordCloud(width=800, height=400, background_color='white').generate(Likes_text)
Dislikes_wordcloud=WordCloud(width=800, height=400, background_color='white').generate(Dislikes_text)

#Plot the 'Likes_wordcloud and Dislikes_worldcloud'
plt.figure(figsize=(10,5))
plt.imshow(Likes_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Likes')
plt.show()

plt.figure(figsize=(10,5))
plt.imshow(Dislikes_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Dislikes')
plt.show()

# Defining a function to remove punctuation
def remove_punctuation(text):
    text = str(text)
    text = re.sub(r'[^\w\s]', '', text)
    return text

# Apply the function to 'Likes' and 'Dislikes' columns
df['Likes'] = df['Likes'].apply(remove_punctuation)
df['Dislikes'] = df['Dislikes'].apply(remove_punctuation)

# Displaying the cleaned dataFrame
print(df[['Likes', 'Dislikes']].head())

"""# Exploratory Data Analysis

The exploratory data analysis involves data visualization  and
applying machine learning models and deep learning models on the dataset to better understand the dataset.

Distribution of Ratings
"""

#Plotting the ratings columns
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Rating', bins=10, kde=True)
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

"""Distribution of Departments"""

# Plotting the top 10 Department column
plt.figure(figsize=(12, 6))

# Getting the top 10 departments
top_10_departments = df['Department'].value_counts().nlargest(10).index

# Filtering the DataFrame to include only the top 10 departments
filtered_df = df[df['Department'].isin(top_10_departments)]

# Define a color palette for the plots
colors = sns.color_palette("viridis", n_colors=10)

# Plot the countplot for the top 10 departments
sns.countplot(data=filtered_df, y='Department', order=top_10_departments, hue='Department', palette=colors, legend=False)

plt.title('Distribution of Top 10 Departments')
plt.xlabel('Count')
plt.ylabel('Department')
plt.show()

"""Job Type Distribution

"""

# Plotting the Job Type column
plt.figure(figsize=(10, 6))

#Plotting the Job Type
sns.countplot(data=df, x='Job_type', order=df['Job_type'].value_counts().index, color='purple')

plt.title('Distribution of Job Types')
plt.xlabel('Job Type')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

"""Distribution of Word Length"""

#Plot the word length for the likes and dislikes column
# Calculate the length of 'Likes' and 'Dislikes' for each row
df['Likes_length'] = df['Likes'].apply(len)
df['Dislikes_length'] = df['Dislikes'].apply(len)

# Create subplots for 'Likes' and 'Dislikes' length distributions
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Plot 'Likes' length distribution
sns.histplot(df['Likes_length'], ax=axes[0], kde=True)
axes[0].set_title('Distribution of Likes Length')
axes[0].set_xlabel('Likes Length')
axes[0].set_ylabel('Frequency')

# Plot 'Dislikes' length distribution
sns.histplot(df['Dislikes_length'], ax=axes[1], kde=True)
axes[1].set_title('Distribution of Dislikes Length')
axes[1].set_xlabel('Dislikes Length')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""Distribution of Top 20 Most Frequent Words"""

# Combine the 'Likes' and 'Dislikes' columns into a new column 'Combined_text'
df['Combined_text'] = df['Likes'].astype(str) + ' ' + df['Dislikes'].astype(str)

# Combine all text from the 'Combined_text' column into a single string
all_text = ' '.join(df['Combined_text'].astype(str).tolist())

# Tokenizing the text and create a frequency distribution
word_frequencies = Counter(all_text.split())

# Showing/filter the most common top 20 words
top_words = word_frequencies.most_common(20)

# Extract words and frequencies for plotting
words = [word for word, frequency in top_words]
frequencies = [frequency for word, frequency in top_words]

plt.figure(figsize=(10, 6))
sns.barplot(x=frequencies, y=words, orient='h')
plt.title('Top 20 Most Frequent Words')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.show()

"""# SENTIMENT ANALYSIS

The dataset has three labels positive, neutral and negative which would be encoded as a numerical labels for machine learning model. The VADER was used to get the sentiment score of each text by calculating the polarity score of the text. The label encoder was used for converting categorical columns that is the 'Postive', 'Neutral', 'Negative' into numerical values.
"""

#Initializing the sentiment intensity analyzer
analyzer = SentimentIntensityAnalyzer()

#Defining a function for the VADER  to get the sentiment scores of the text
def get_sentiment_scores(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']

df['Sentiment_Score'] = df['Combined_text'].apply(get_sentiment_scores)

df['Sentiment_Label'] = df['Sentiment_Score'].apply(lambda score: 'Positive' if score >= 0.05 else 'Negative' if score <= -0.05 else 'Neutral')

print(df[[ 'Combined_text','Sentiment_Score' ]].head(10))

#Defining a function for the label encoder to convert catergicals 'Postive', 'Neutral' and 'Negative' to numerical
def sentiment_encoded(df):
    # Initialize the LabelEncoder
    label_encoder = LabelEncoder()

    # Apply label encoding to the 'Sentiment' column
    df['Encoded_sentiment'] = label_encoder.fit_transform(df['Sentiment'])

    # Display the original and encoded sentiment columns
    return df[['Sentiment', 'Encoded_sentiment']]


encodedlabels = sentiment_encoded(df)
print(encodedlabels)

# Plot and display the sentiment label
plt.figure(figsize=(8, 5))

# Plot the sentiment distribution
sns.countplot(data=df, x='Sentiment', order=df['Sentiment'].value_counts().index,
              hue='Sentiment', palette=['green', 'blue', 'red'], legend=False)

plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.show()

# Accessing the 'Sentiment' column using df['Sentiment']
sentiment_counts = df['Sentiment'].value_counts()
sentiment_counts

"""#Text Vectorization

Here, the text data is converted into numerical formats that machine learning models can understand using the TF-IDF Vectorization. TF-IDF gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a dataset.

TF-IDF
"""

# Encode the sentiment labels
encodedlabels = sentiment_encoded(df)

# Combine the 'Likes' and 'Dislikes' columns into a new column 'Combined_text'
df['Combined_text'] = df['Likes'].astype(str) + ' ' + df['Dislikes'].astype(str)

# Define features (X) and target (y)
# The likes and dislikes
X = df['Combined_text']
 # Positive, Negative, Neutral Sentiment labels
y = df['Encoded_sentiment']

# Split data into training and testing sets (80% for training, 20% for testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

# Initialize the TF-IDF vectorizer with a maximum of 4000 features
tfidf = TfidfVectorizer(max_features=4000)

# Fit and transform the training data, then transform the test data
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

"""Three machine learning models was applied to compare the trained model accuracy and two deep learing models was applied

Machince Learning Model: Multinomial Naïve Bayes Bayes, Support Vector Machine (SVM), Random Forest.

Deep Learning Model: Long Short-term Memory(LSTM), Convolutional Neural Network and Recurrent Neural Network

The Multinomial Naïve Bayes Bayes Classifier

The Multinomial Naïve Bayes Bayes Classifier is machine learning model used to train and learn the datasets. This approach features can estimate the likelihood of a word to be positive or negative which is why its suitaability for this analysis
"""

# Apply and train Multinomial Naïve Bayes model
# Using the smaller alpha for less smoothing to avoid zero probability
naive_bayes = MultinomialNB(alpha=0.5)
naive_bayes.fit(X_train_tfidf, y_train)

# Predict and evaluate the model
train_acc_nb = accuracy_score(y_train, naive_bayes.predict(X_train_tfidf))
test_acc_nb = accuracy_score(y_test, naive_bayes.predict(X_test_tfidf))

#Print the accuracy result
print(f"Naive Bayes Train Accuracy: {train_acc_nb}")
print(f"Naive Bayes Test Accuracy: {test_acc_nb}")

# Defining the labels and accuracies for the plot
labels = ['Train', 'Test']
accuracies = [train_acc_nb, test_acc_nb]

# Plotting the results
plt.figure(figsize=(6, 4))
sns.barplot(x=labels, y=accuracies, hue=labels, palette=['blue', 'green'], legend=False)
plt.ylim([0, 1])
plt.title('Naive Bayes Model Accuracy')
plt.ylabel('Accuracy')
plt.show()

"""Support Vector Machine (SVM) is an another machine learning algorithm that was used to train the dataset. It identifies the ideal hyperplane between the positive and negative sentiment labels

"""

# Calling up  the encode_sentiment_column function with the DataFrame 'df' as argument
encodedlabels = sentiment_encoded(df)
#Accessing the label encoder to access the class attribute
label_encoder = LabelEncoder()
label_encoder.fit(df['Sentiment'])

# Train the SVM model with a linear kernel
model = SVC(kernel='linear')
model.fit(X_train_tfidf, y_train)

# For model predictions
y_pred = model.predict(X_test_tfidf)

# Convert predicted labels back to sentiment labels
y_pred_labels = label_encoder.inverse_transform(y_pred)
y_test_labels = label_encoder.inverse_transform(y_test)

# Calculate accuracy
accuracy = accuracy_score(y_test_labels, y_pred_labels)
print(f"Accuracy: {accuracy}")


# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix for SVM Model')
plt.show()

# Print classification report
# Convert label_encoder.classes_ to a list of strings
target_names = label_encoder.classes_.tolist()
classification_rep = classification_report(y_test, y_pred, target_names=target_names)
print(classification_rep)

"""Random Forest Classifier

Random Forest is able to classify large dataset with accuracy. It is made up of mulitple decision trees.
"""

# Call the encode_sentiment_column function with the DataFrame 'df' as argument
encodedlabels = sentiment_encoded(df)

#Accessing the label encoder to access the class attribute
label_encoder = LabelEncoder()
label_encoder.fit(df['Sentiment'])

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=101)

# Train the Random Forest model
rf_model.fit(X_train_tfidf, y_train)

# Predict the sentiment on the test set
y_pred_rf = rf_model.predict(X_test_tfidf)

# Evaluate the Random Forest model
classification_rep_rf = classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_)
print(classification_rep_rf)

# Confusion matrix for Random Forest model
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

# Plot confusion matrix
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix for Random Forest Model')
plt.show()

"""# Deep Learning Model

In order to conduct a deep learning model, the Word2Vec embedding model was used to convert the text to numerical format

Word2Vec
"""

# Tokenize the sentences in 'Combined_text' and store them in 'sentences_tokenized'
sentences_tokenized = df['Combined_text'].apply(clean_text).tolist()

# Create and train a Word2Vec model, sentences_tokenized is a list of lists, each inner list representing a sentence tokenized into words.
model_w2v = Word2Vec(sentences_tokenized, vector_size=100, window=5, min_count=1, workers=4)

# Using bracket notation to access word vectors instead of .get() and shaping word embeddings to be 2D for compatibility with LSTM
sentence_vectors = [
    np.array([model_w2v.wv[word] if word in model_w2v.wv else np.zeros(model_w2v.vector_size) for word in sentence]).reshape(-1, model_w2v.vector_size)
    for sentence in sentences_tokenized]

# Pad the sequences so that they are all of the same length
max_length = max(len(sentence) for sentence in sentence_vectors)
sentence_vectors_padded = pad_sequences(sentence_vectors, maxlen=max_length, dtype='float32', padding='post')

"""Recurrent Neural Network"""

# Call the encode_sentiment_column function with the DataFrame 'df' as argument
encodedlabels = df['Encoded_sentiment'].values

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(sentence_vectors_padded, encodedlabels, test_size=0.2, random_state=42)

# Defining the SimpleRNN
model = Sequential()

# Input layer to define the input shape
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))

#SimpleRNN layer
model.add(SimpleRNN(50, activation='tanh'))

# Dropout layer for regularization
model.add(Dropout(0.6))

# Dense layer for multi-class classification (3 classes)
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Define EarlyStopping callback to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Fit the model with training data, validation data, and EarlyStopping
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), shuffle=False, callbacks=[early_stopping])


# Evaluate the model on the test set
accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy[1] * 100:.2f}%")

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
sns.lineplot(x=range(1, len(history.history['accuracy']) + 1), y=history.history['accuracy'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_accuracy']) + 1), y=history.history['val_accuracy'], label='Validation')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
sns.lineplot(x=range(1, len(history.history['loss']) + 1), y=history.history['loss'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_loss']) + 1), y=history.history['val_loss'], label='Validation')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()

"""Long Short Term Memory


Long Short-Term Memory(LSTM) is a type of RNN that help capture long-range dependencies in sequences. This model was applied to the dataset

"""

# Get the encoded sentiment labels
encodedlabels = df['Encoded_sentiment'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(sentence_vectors_padded, encodedlabels, test_size=0.2, random_state=42)

# Define the model
model = Sequential()

# Input layer to define the input shape
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))

# Creating the Bidirectional LSTM Layer with 100 units and ReLU activation
model.add(Bidirectional(LSTM(100, activation='relu')))

# Add a Dropout layer for regularization
model.add(Dropout(0.3))

# Dense Layer for Classification, output size is 3 for sentiment classes
model.add(Dense(3, activation='softmax'))

# Compile the model using Adam optimizer and sparse categorical crossentropy loss
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with a higher batch size using epochs 30 for faster training
history = model.fit( X_train,  y_train,  epochs=30, batch_size=256,  validation_data=(X_test, y_test),
                    callbacks=[early_stopping], verbose=1 )

# Evaluate on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))

# Create a subplot for accuracy
plt.subplot(1, 2, 1)
sns.lineplot(x=range(1, len(history.history['accuracy']) + 1), y=history.history['accuracy'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_accuracy']) + 1), y=history.history['val_accuracy'], label='Validation')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
sns.lineplot(x=range(1, len(history.history['loss']) + 1), y=history.history['loss'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_loss']) + 1), y=history.history['val_loss'], label='Validation')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()

"""Convolutional Neural Network

"""

# Get the encoded sentiment labels
encodedlabels = df['Encoded_sentiment'].values

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(sentence_vectors_padded, encodedlabels, test_size=0.2, random_state=42)

# Adding a Limit sequence length for faster training
max_length = 100
X_train = np.array([x[:max_length] for x in X_train])
X_test = np.array([x[:max_length] for x in X_test])

# Pad sequences to ensure they are of equal length

X_train = pad_sequences(X_train, maxlen=max_length)
X_test = pad_sequences(X_test, maxlen=max_length)

# Convert labels to categorical for multi-class classification
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Defining embedding dim and getting the embedding dimension from X_train
embedding_dim = X_train.shape[2]

# Create the CNN model using the Functional API
inputs = Input(shape=(max_length, embedding_dim))
x = Conv1D(filters=32, kernel_size=3, activation='relu')(inputs)
x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)
x = MaxPooling1D(pool_size=2)(x)
x = Flatten()(x)
outputs = Dense(3, activation='softmax')(x)

# Creating the model
model = Model(inputs=inputs, outputs=outputs)


# Compile the model with categorical_crossentropy
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Summarize the model
model.summary()

# Fit the model and store the history with early stopping
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=256, verbose=2, callbacks=[early_stopping])

# Final evaluation of the model
scores = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1] * 100))

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
sns.lineplot(x=range(1, len(history.history['accuracy']) + 1), y=history.history['accuracy'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_accuracy']) + 1), y=history.history['val_accuracy'], label='Validation')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
sns.lineplot(x=range(1, len(history.history['loss']) + 1), y=history.history['loss'], label='Train')
sns.lineplot(x=range(1, len(history.history['val_loss']) + 1), y=history.history['val_loss'], label='Validation')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()